<!-- vscode-markdown-toc -->
* [并行程序设计 上机练习一](#)
	* [实验一](#-1)
		* [二叉树求和](#-1)
		* [蝶式求和](#-1)
	* [实验二](#-1)
		* [题目](#-1)
		* [作业1.3.1](#1.3.1)
		* [作业3.3.2](#3.3.2)
		* [作业3.5.1](#3.5.1)
		* [作业3.5.2](#3.5.2)
	* [实验三](#-1)
		* [题面](#-1)
		* [代码实现](#-1)
		* [运行截图:](#:)
	* [实验四](#-1)
		* [题面](#-1)
		* [代码实现:](#:-1)
		* [运行截图](#-1)
* [并行程序设计 上机练习二](#-1)
	* [实验一(LU分解):](#LU:)
		* [题目](#-1)
		* [代码实现](#-1)
		* [运行截图:](#:-1)
	* [实验二(QR分解)](#QR)
		* [题目](#-1)
		* [代码实现](#-1)
		* [运行截图](#-1)
	* [实验三(summa)](#summa)
		* [题目:](#:-1)
		* [代码实现:](#:-1)
		* [运行截图](#-1)
	* [实验四(自选命题-随机算法并行实现)](#-)
		* [题目](#-1)
		* [代码实现](#-1)
		* [运行截图](#-1)

<!-- vscode-markdown-toc-config
	numbering=false
	autoSave=false
	/vscode-markdown-toc-config -->
<!-- /vscode-markdown-toc -->
<div STYLE="page-break-after: always;"></div>

## <a name=''></a>并行程序设计 上机练习一 
### <a name='-1'></a>实验一
**题目:** 课程主页lec2_PP.ppt的P35-36两种重复计算方式的OpenMP和MPI实现。（任务数可约定为2的幂次方）
#### <a name='-1'></a>二叉树求和
##### <a name=':'></a>算法流程图:
![](./pic/1.png)
##### <a name='PCAM'></a>PCAM分析
**划分:**
域分解:每个节点均包含一个当前节点权值,均为一个域
功能分解:将任务分成三种,叶子节点,中间节点,根节点
**通信:**
使用静态的结构化异步通信
对于叶子节点,完成两次通信,第一次向其父节点发送该叶子节点的值,第二次等待父亲节点传回sum
对于中间节点,完成三次通信,第一次所有接受子节点的和,第二次等待其父节点回传sum,第三次通信发送收到的sum值给所有儿子节点
对于根节点完成两次通信,第一次通信获得所有子节点的和,第二次将计算的sum发送给所有儿子节点
**组合:**
我们将任务分成三种,因此我们对与每个节点均以自己作为一个组合(不划分)
>+ 对于叶子节点在第二次通信时将节点自身值赋为sum
>+ 对于根节点,在第二次通信前计算所有子节点传来的数值的和,并加上本身节点值
>+ 对于中间节点,在第一次通信以及第二次通信前计算所有子节点传来的数值的和,并加上本身节点值

**映射:**
将对应的任务分别发送给叶子节点/中间节点/根节点

##### <a name=':-1'></a>代码实现:
```c++
#include <stdio.h>
#include <string.h>
#include <bits/stdc++.h>
#include "mpi.h"
#define GHH(...) printf(__VA_ARGS__)
// #define GHH(...) 
// using namespace std;

int main(int argc, char* argv[])
{
    clock_t begin = clock();
    int numprocs, myid, source;
    MPI_Status status;
    int data[10];
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);

    data[0] = myid;
    GHH("thread %d begin!\n",myid);
    if(myid == 0);
    else if(myid == 1 ){                 //根节点
        MPI_Recv(data+1, 1, MPI_INT, myid*2, myid*2,  //阶段1,收集来自左右子树的和
            MPI_COMM_WORLD, &status);
        GHH("root %d get sum %d from left son %d\n",myid,data[1],myid*2);
       
        MPI_Recv(data+2, 1, MPI_INT, myid*2+1, myid*2+1,
            MPI_COMM_WORLD, &status);
        GHH("root %d get sum %d from right son %d\n",myid,data[2],myid*2+1);
        
        data[0]+=data[1]+data[2];

        GHH("root %d send sum %d to left son %d\n",myid,data[0],myid*2);
        MPI_Send(data, 1, MPI_INT, myid*2, myid,    //阶段2,向下广播求和的值
            MPI_COMM_WORLD); 

        GHH("root %d send sum %d to right son %d\n",myid,data[0],myid*2+1);
        MPI_Send(data, 1, MPI_INT, myid*2+1, myid,    //阶段2,向下广播求和的值
            MPI_COMM_WORLD); 
    }else if (myid >= (numprocs+1)/2){ //叶子结点
        MPI_Send(data, 1, MPI_INT, myid/2, myid,    //阶段1,向上传递自身节点值
            MPI_COMM_WORLD); 
        GHH("leaf %d send itself to parent %d\n",myid,myid/2);
        
        MPI_Recv(data, 1, MPI_INT, myid/2, myid/2,    //阶段2,收到来自根节点的全局和
            MPI_COMM_WORLD, &status);
        GHH("leaf %d get sum %d from parent %d\n",myid,data[0],myid/2);

    }else {                         //中间节点
        MPI_Recv(data+1, 1, MPI_INT, myid*2, myid*2,  //阶段1,收集来自左右子树的和
            MPI_COMM_WORLD, &status);        
        GHH("midnode %d get sum %d from left son %d\n",myid,data[1],myid*2);

        MPI_Recv(data+2, 1, MPI_INT, myid*2+1, myid*2+1,
            MPI_COMM_WORLD, &status);
        GHH("midnode %d get sum %d from right son %d\n",myid,data[2],myid*2+1);

        data[0]+=data[1]+data[2];

        MPI_Send(data, 1, MPI_INT, myid/2, myid,    //向其父亲节点发送该节点左右子树以及自己的和
            MPI_COMM_WORLD); 
        GHH("midnode %d send itself to parent %d\n",myid,myid/2);

        MPI_Recv(data, 1, MPI_INT, myid/2, myid/2,    //阶段2,向下广播自根节点的全局和
            MPI_COMM_WORLD, &status);
        GHH("midnode %d get sum %d from parent %d\n",myid,data[0],myid/2);

        MPI_Send(data, 1, MPI_INT, myid*2, myid,
            MPI_COMM_WORLD);
        GHH("midnode %d send sum %d to left son %d\n",myid,data[0],myid*2);

        MPI_Send(data, 1, MPI_INT, myid*2+1, myid,
            MPI_COMM_WORLD);
        GHH("midnode %d send sum %d to right son %d\n",myid,data[0],myid*2+1);

    }
    MPI_Finalize();

    clock_t end = clock();
    printf("thread %d sum is %d. Runtime :%lf(ms)\n",myid,data[0],1000.0*(end-begin)/CLOCKS_PER_SEC);
} /* end main */

// mpic++ -o fun test.cpp -fopenmp > compile.log
// mpirun -n 8 -genv OMP_NUM_THREADS 1 ./fun > out.log
```

##### <a name='-1'></a>运行结果展示
![](./pic/3.png)

#### <a name='-1'></a>蝶式求和
##### <a name='-1'></a>算法流程图
![](./pic/2.png)
##### <a name='PCAM-1'></a>PCAM分析
**划分:**
域分解:每个计算节点均等价,因此每个计算节点本身就是一个划分
功能分解:每个计算节点均等价,因此每个计算节点功能一致,仅有一个划分
**通信:**
使用动态的非结构化同步通信
使用同步的方式,记录当前节点id为i(从0到n-1),第k轮将节点i给id为$i\oplus2^k(\oplus 表示异或)$的节点发送当前节点sum
**组合:**
由于任务只有一种,因此我们仅需要叙述每个节点对应的步骤:
>+ 首先使用同步的通信方式,所有节点按轮完成其对应的数据传输任务(保证数据一致性)
>+ 对于第k轮将节点i给id为$i\oplus2^k(\oplus 表示异或)$的节点发送当前节点sum
>+ 保证每个节点均接受到该轮传输的对应$sum_{old}$之后,更新本身节点值$sum=sum+sum_{old}$

**映射:**
将上述组合后的任务分发至所有节点即可
##### <a name='-1'></a>代码实现
```c++
#include <stdio.h>
#include <string.h>
#include <bits/stdc++.h>
#include "mpi.h"
int main(int argc, char* argv[])
{
    clock_t begin = clock();
    int numprocs, myid, source;
    MPI_Status status;
    int data[10];
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);
    data[0]=myid+1;
    for(int i = 1; i < numprocs ; i<<=1){
        MPI_Send(data,1,MPI_INT,myid^i,myid^i,MPI_COMM_WORLD);
        MPI_Recv(data+1,1,MPI_INT,myid^i,myid,MPI_COMM_WORLD,&status);
        data[0] += data[1];
    }
    MPI_Finalize();
    clock_t end = clock();
    printf("thread %d sum is %d. Runrime:%lf(ms)\n",myid,data[0],1000.0*(end-begin)/CLOCKS_PER_SEC);
} /* end main */

// mpic++ -o fun test.cpp -fopenmp > compile.log
// mpirun -n 8 -genv OMP_NUM_THREADS 1 ./fun > out.log
```
##### <a name='-1'></a>运行结果展示
![](./pic/4.png)

### <a name='-1'></a>实验二
#### <a name='-1'></a>题目
前期练习作业题目中的相关程序实现（ex-21-1/2/3 中有要求向量化/并行化的程序实习）。
#### <a name='1.3.1'></a>作业1.3.1
##### <a name=':-1'></a>题面:
**3. 向量化以下循环。如果不能，请说明原因**。
```c++
(1) for I = 1 to N do 
        S:A(I) = B(I) + C(I+1); 
        T:C(I) = A(I)* D(I); 
    end for 
```
```c++
    S:A(1,N) = B(I:N) + C(2:N+1); 
    T:C(1:N) = A(1:N) * D(1:N); 
```
```c++
(2) for I = 1 to N do 
        S:A(I) = A(I-1) + 1 
    end for 
```
>存在依赖$S\delta^fS$方向向量为(1)因此不能并行化

##### <a name=':-1'></a>代码实现:
```c++
#include <bits/stdc++.h>
#include "mpi.h"
using namespace std;
const int n = 1000000;
int a[n+10],b[n+10],c[n+10],d[n+10];
int aa[n+10],bb[n+10],cc[n+10],dd[n+10];
void chuan(){
    clock_t begin = clock();
    for(int i =1;i<=n;i++){
        aa[i]=bb[i]+cc[i+1];
    }
    for(int i=1;i<=n;i++){
        cc[i]=aa[i]*dd[i];
    }
    clock_t end = clock();
    printf("串行用时:%5lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);
}
void check(){
    for(int i=1;i<=n;i++){
        if(aa[i]!=a[i]||cc[i]!=c[i]) cout<<"error!"<<endl;
    }
}
int main(int argc, char* argv[])
{
    
    for(int i=0;i<n+5;i++){
        a[i]=aa[i]=rand()%100;
        b[i]=bb[i]=rand()%100;
        c[i]=cc[i]=rand()%100;
        d[i]=dd[i]=rand()%100;

    }
    chuan();

    clock_t begin = clock();
    #pragma omp parallel for 
    for(int i =1;i<=n;i++){
        a[i]=b[i]+c[i+1];
    }
    #pragma omp parallel for 
    for(int i=1;i<=n;i++){
        c[i]=a[i]*d[i];
    }
    clock_t end = clock();
    printf("并行用时:%5lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);

    check();
} /* end main */
```
##### <a name='-1'></a>运行结果展示
当OMP_NUM_THREADS 设置为2时运行截图如下:
![](./pic/7.png)
当OMP_NUM_THREADS 设置为4时运行截图如下:
![](./pic/5.png)
当OMP_NUM_THREADS 设置为8时运行截图如下:
![](./pic/6.png)

因此我们可以得出,需要设置合适的OMP_NUM_THREADS才能发挥并行的优势

#### <a name='3.3.2'></a>作业3.3.2
##### <a name='-1'></a>题面
(2) 尝试向量化/并行化此循环。
```c++
for i = 1 to 100 do // 循环 2 N 是常量
    for j = 1 to 100 do
        S2:B[j] = A[j, N]; // 语句 S2
        doall k = 1 to 100 do 
            S3:A[j+1, k] = B[j] + C[j, k]; // 语句 S3
        enddoall // loop-k
        S4:Y[i+j] = A[j+1, N]; // 语句 S4
    endfor // loop-j
endfor // loop-i

doall i = 1 to 100 do
    S1:X[i] = Y[i] + 10; // 语句 S1
enddoall 
```
##### <a name='-1'></a>代码实现
```c++
#include <bits/stdc++.h>
#include "mpi.h"
using namespace std;
const int n = 100;
int a[n+10][n+10],b[n+10],c[n+10][n+10],d[n+10];
int aa[n+10][n+10],bb[n+10],cc[n+10][n+10],dd[n+10];
int y[n*2+10],x[n*2+10];
int yy[n*2+10],xx[n*2+10];
double chuan(){         //串行程序计算
    clock_t begin = clock();
    for(int i =1;i<=n;i++){
        xx[i]=yy[i]+10;
        for(int j =1;j<=n;j++){
            bb[j]=aa[j][n];
            for(int k=1;k<=n;k++){
                aa[j+1][k]=bb[j]+cc[j][k];
            }
            yy[i+j]=aa[j+1][n];
        }
    }
    clock_t end = clock();
    // printf("串行用时:%5lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);
    return 1000.0*(end-begin)/CLOCKS_PER_SEC;
}
bool check(){           //检查是否与串行程序相同
    for(int i=1;i<=n;i++){
        for(int j =1;j<=n;j++)
            if(aa[i][j]!=a[i][j]||cc[i][j]!=c[i][j]) {
                cout<<"error!"<<endl;
                return false;
            }
        if(b[i]!=bb[i]||d[i]!=dd[i]||x[i]!=xx[i]||y[i]!=yy[i]) {
            cout<<"error"<<endl;
            return false;
        }
    }
    return true;
}
void randData(){        //生成随机数据
    for(int i=0;i<n+5;i++){
        for(int j =0;j<n+5;j++) {
            a[i][j]=aa[i][j]=rand()%100;
            c[i][j]=cc[i][j]=rand()%100;
        }
        b[i]=bb[i]=rand()%100;
        d[i]=dd[i]=rand()%100;
        x[i]=xx[i]=rand()%100;
        y[i]=yy[i]=rand()%100;
    }
}
int main(int argc, char* argv[])
{
    randData();      //生成随机数据

    double time_seq = chuan();

    clock_t begin = clock();
    for(int i =1;i<=n;i++){
        for(int j = 1;j<=n;j++){
            b[j]=a[j][n];
            // #pragma omp parallel for 
            for(int k =1;k<=n;k++){
                a[j+1][k]=b[j]+c[j][k];
            }
            y[i+j] = a[j+1][n];
        }
    }
    
    #pragma omp parallel for 
    for(int i=1;i<=n;i++){
        x[i]=y[i]+10;
    }
    clock_t end = clock();

    if(check()) printf("串行程序与并行程序结果一致!\n");
    double time_pra = 1000.0*(end-begin)/CLOCKS_PER_SEC;
    printf("并行用时:%5lf(ms)\n",time_pra);
    printf("串行用时:%5lf(ms)\n",time_seq);
    printf("加速比为:%lf\n",time_seq/time_pra);

} /* end main */
```

##### <a name='-1'></a>运行结果展示
![](./pic/8.png)

#### <a name='3.5.1'></a>作业3.5.1
##### <a name='-1'></a>题面
五、 分析以下 3 个循环中存在的依赖关系；分别通过循环交换、分布和逆转
等多种方法来尝试向量化和/或并行化变换：
```c++
for i = 1 to 100 do //循环 1 
    S:A[i] = A[i] + B[i-1];
    T:B[i] = C[i-1] * 2 ;
    U:C[i] = 1 / B[i] ;
    V:D[i] = C[i] * C[i] ;
endfor
```
##### <a name=':-1'></a>代码实现:
```c++
#include <bits/stdc++.h>
#include "mpi.h"
using namespace std;
const int n = 1000000;
const int mod = 1e4;
int a[n+10],b[n+10],c[n+10],d[n+10];
int aa[n+10],bb[n+10],cc[n+10],dd[n+10];
double chuan(){             //串行程序计算
    clock_t begin = clock();
    for(int i=1;i<=n;i++){
        aa[i]=aa[i]+bb[i-1];
        bb[i]=cc[i-1]*2%mod+1;
        cc[i]=int(1/bb[i]);
        dd[i]=cc[i]*cc[i]%mod+1;
    }
    clock_t end = clock();
    // printf("串行用时:%5lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);
    return 1000.0*(end-begin)/CLOCKS_PER_SEC;
}
bool check(){
    for(int i=1;i<=n;i++){
        if(aa[i]!=a[i]||cc[i]!=c[i]||bb[i]!=b[i]||dd[i]!=d[i]) {
            cout<<"error!"<<endl;
            return false;
        }
    }
    return true;
}
void randData(){             //生成随机数据
    for(int i=0;i<n+5;i++){
        a[i]=aa[i]=rand()%100;
        b[i]=bb[i]=rand()%100;
        c[i]=cc[i]=rand()%100;
        d[i]=dd[i]=rand()%100;
    }
}
int main(int argc, char* argv[])
{
    randData();          //生成随机数据

    double time_seq = chuan();  //串行程序计算

    clock_t begin = clock();
    for(int i =1;i<=n;i++){
        b[i]=c[i-1]*2%mod+1;
        c[i]=int(1/b[i]);
    }
    #pragma omp parallel for 
    for(int i=1;i<=n;i++){
        a[i]=a[i]+b[i-1];
        d[i]=c[i]*c[i]%mod+1;
    }
    clock_t end = clock();

    if(check()) printf("串行程序与并行程序结果一致!\n");
    double time_pra = 1000.0*(end-begin)/CLOCKS_PER_SEC;
    printf("并行用时:%5lf(ms)\n",time_pra);
    printf("串行用时:%5lf(ms)\n",time_seq);
    printf("加速比为:%lf\n",time_seq/time_pra);

} /* end main */
```
##### <a name='-1'></a>运行结果展示
![](./pic/9.png)

由于该算法比较简单,在数据规模较小时无论怎么设置
#### <a name='3.5.2'></a>作业3.5.2
##### <a name='-1'></a>题面
```c++
for i = 1 to 999 do // 循环 2
    S:A[i] = B[i] + C[i];
    T:D[i] = ( A[i] + A[ 999-i+1 ] ) / 2 ;
endfor
```
##### <a name='-1'></a>代码实现
```c++
#include <bits/stdc++.h>
#include "mpi.h"
using namespace std;
const int n = 99999;
const int mod = 1e4;
int a[n+10],b[n+10],c[n+10],d[n+10];
int aa[n+10],bb[n+10],cc[n+10],dd[n+10];
double chuan(){
    clock_t begin = clock();
    for(int i=1;i<=n;i++){
        aa[i]=bb[i]+cc[i];
        dd[i]=(aa[i]+aa[n-i+1])/2;
    }
    clock_t end = clock();
    // printf("串行用时:%5lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);
    return 1000.0*(end-begin)/CLOCKS_PER_SEC;
}
bool check(){
    for(int i=1;i<=n;i++){
        if(aa[i]!=a[i]||cc[i]!=c[i]||bb[i]!=b[i]||dd[i]!=d[i]) {
            cout<<i<<" "<<"error!"<<endl;
            return false;
        }
    }
    return true;
}
int main(int argc, char* argv[])
{
    
    for(int i=0;i<n+5;i++){         
        a[i]=aa[i]=rand()%100;
        b[i]=bb[i]=rand()%100;
        c[i]=cc[i]=rand()%100;
        d[i]=dd[i]=rand()%100;

    }

    double time_seq = chuan();

    clock_t begin = clock();
    #pragma omp parallel for 
    for(int i =1;i<=(n+1)/2;i++){
        a[i]=b[i]+c[i];
        // d[i]=(a[i]+a[n-i+1])/2;
    }
    #pragma omp parallel for 
    for(int i =1;i<=(n+1)/2;i++){
        d[i]=(a[i]+a[n-i+1])/2;
    }
    #pragma omp parallel for 
    for(int i=(n+1)/2+1;i<=n;i++){
        a[i]=b[i]+c[i];
        // d[i]=(a[i]+a[n-i+1])/2;
    }
    #pragma omp parallel for 
    for(int i=(n+1)/2+1;i<=n;i++){
        d[i]=(a[i]+a[n-i+1])/2;
    }
    clock_t end = clock();

    if(check()) printf("串行程序与并行程序结果一致!\n");
    double time_pra = 1000.0*(end-begin)/CLOCKS_PER_SEC;
    printf("并行用时:%5lf(ms)\n",time_pra);
    printf("串行用时:%5lf(ms)\n",time_seq);
    printf("加速比为:%lf\n",time_seq/time_pra);
} /* end main */
``` 
##### <a name='-1'></a>运行结果展示
![](./pic/11.png)

### <a name='-1'></a>实验三
#### <a name='-1'></a>题面
新的广播MyBcastMPI实现。基本思路：
（1）将MPI进程按所在节点划分子通讯域N；
（2）可以将各子通讯域的首进程（编号为0）再组成一个子通讯域H；
（3）由广播的root进程将消息发给原来最大通讯域中的0号进程h，再由h在H通讯域中广播（MPI_Bcast），各首进程然后在各自子通讯域N中再行广播（MPI_Bcast）。 
#### <a name='-1'></a>代码实现
```c++
// 本实验通信域大小为4,其中所有myid%4=0的构成通信域H
// root节点为id最大的,因此要求numprocs应该为4k+1
#include <stdio.h>
#include <string.h>
#include "mpi.h"
int data[10];
int main(int argc, char* argv[])
{
    int numprocs, myid, source;
    MPI_Status status;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);


    if(myid == numprocs-1){ //为root节点
        data[0]=100;
        printf("root node send msg to h0\n");
        MPI_Send(data, 1, MPI_INT, 0, 0,MPI_COMM_WORLD); 
    }else if(myid%4 == 0){  //设定myid%4==0的节点构成子通信域N
        if(myid==0){        //myid=0的为0号进程
            MPI_Recv(data, 1, MPI_INT, numprocs-1, myid,
                MPI_COMM_WORLD, &status);
            printf("h0 node received msg %d\n",data[0]);
            for(int i = myid+4; i <= numprocs-1 ; i+=4){
                printf("h0 node send msg to h%d\n",i/4);
                MPI_Send(data, 1, MPI_INT, i, i,MPI_COMM_WORLD);
            }
        }else{
            MPI_Recv(data, 1, MPI_INT, 0, myid,
                MPI_COMM_WORLD, &status);
            printf("h%d node received msg %d\n",myid/4,data[0]);
        }
        for(int i = 1; i <= 3; i++){
            printf("h%d node send msg to n%d%d\n",myid/4,myid/4,i);
            MPI_Send(data, 1, MPI_INT, myid+i, myid+i,
                MPI_COMM_WORLD);
        }
    }else {         //通信域N收到消息
        MPI_Recv(data, 1, MPI_INT, myid-myid%4, myid,
                MPI_COMM_WORLD, &status);
        printf("n%d%d node received msg %d\n",myid/4,myid%4,data[0]);
    }
    MPI_Finalize();
} /* end main */
```
#### <a name=':'></a>运行截图:
![](./pic/12.png)
### <a name='-1'></a>实验四
#### <a name='-1'></a>题面
用MPI_Send和MPI_Recv来模拟实现诸如MPI_Alltoall, MPI_Allgather功能并与标准MPI实现做简要性能对比
#### <a name=':-1'></a>代码实现:
```c++
// 本实验通信域大小为4,其中所有myid%4=0的构成通信域H
// root节点为id最大的,因此要求numprocs应该为4k+1
#include <stdio.h>
#include <string.h>
#include <bits/stdc++.h>
#include "mpi.h"
using namespace std;
int data[1050];
int recv[1050];
void my_alltoall(const void *sendbuf, const int sendcount, MPI_Datatype sendtype, 
    void *recvbuf,const int recvcount, MPI_Datatype recvtype, MPI_Comm comm,int numprocs,int myid){
        int offset = 0;
        MPI_Status status;
        // printf("numprocs: %d\n",numprocs);
        for(int i=0;i<numprocs;i++) {
            if(i==myid) continue;
            MPI_Send((int*)(sendbuf)+i*sendcount, sendcount, sendtype, i, 1, comm);
            // printf("thread %d send msg to thread %d\n",myid,i);
        }
        for(int i=0;i<numprocs;i++) {
            if(i==myid) continue;
            MPI_Recv((int*)(recvbuf)+i*recvcount, recvcount, recvtype, i, 1, comm, &status);
        }

        // printf("thread %d recived data :\n",myid);
        // for(int i=0;i<numprocs*recvcount;i++){
        //     printf("%d ",*((int*)(recvbuf)+i*recvcount));
        // }
        // printf("\n");
}
void my_allgather(const void *sendbuf, const int sendcount, MPI_Datatype sendtype, 
    void *recvbuf,const int recvcount, MPI_Datatype recvtype, MPI_Comm comm,int numprocs,int myid){
        int offset = 0;
        MPI_Status status;
        for(int i=0;i<numprocs;i++) {
            if(i==myid) continue;
            MPI_Send((int*)sendbuf, sendcount, sendtype, i, 1, comm);
        }
        for(int i=0;i<numprocs;i++) {
            if(i==myid) continue;
            MPI_Recv((int*)(recvbuf)+i*recvcount, recvcount, recvtype, i, 1, comm, &status);
        }

        // printf("thread %d recived data :\n",myid);
        // for(int i=0;i<numprocs*recvcount;i++){
        //     printf("%d ",*((int*)(recvbuf)+i*recvcount));
        // }
        // printf("\n");
}
int main(int argc, char* argv[])
{
    int numprocs, myid, source;
    MPI_Status status;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);
    for(int i=0;i<10;i++) data[i]=myid*10+i;

    clock_t begin = clock();
    MPI_Alltoall(data,1,MPI_INT,recv,1,MPI_INT,MPI_COMM_WORLD);
    clock_t end = clock();
    if(myid == 0) printf("MPI_Alltoall 用时%.3lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);

    begin = clock();
    my_alltoall(data,1,MPI_INT,recv,1,MPI_INT,MPI_COMM_WORLD,numprocs,myid);
    end = clock();
    if(myid == 0) printf("my_alltoall 用时%.3lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);

    begin = clock();
    MPI_Allgather(data,1,MPI_INT,recv,1,MPI_INT,MPI_COMM_WORLD);
    end = clock();
    if(myid == 0) printf("MPI_AllGather 用时%.3lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);

    begin = clock();
    my_allgather(data,1,MPI_INT,recv,1,MPI_INT,MPI_COMM_WORLD,numprocs,myid);
    end = clock();
    if(myid == 0) printf("my_allgather 用时%.3lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);

    MPI_Finalize();
} /* end main */
```
#### <a name='-1'></a>运行截图
![](./pic/13.png)

虽然我们写的alltoall以及allgather运行速度比标准mpi的快,但是主要考虑到我们没有做必要的类型检查以及类似的程序鲁棒性保障,对于死锁等问题也仅采用简单控制,因此总体上来说并没有标准mpi完备.


## <a name='-1'></a>并行程序设计 上机练习二
### <a name='LU:'></a>实验一(LU分解):
#### <a name='-1'></a>题目
在教材中 18.5 节 LU 分解的并行 MPI 实现基础上，给出 MPI+OpenMP混合实现
#### <a name='-1'></a>代码实现
由于源代码太长了,因此此处并不放源代码仅对重要步骤进行说明:
由于在教材给出的LU分解,因此对于原始算法并没有太大的改动,主要是用OpenMPI进行局部并行化,这里我们主要观察下面代码:
```c++
for(i=1;i<p;i++){
    for(j=0;j<m;j++){
        MPI_Recv(&a(j,0),M,MPI_FLOAT,i,j,MPI_COMM_WORLD,&status);
        for(k=0;k<M;k++)
            A((j*p+i),k)=a(j,k);
    }
}
```
这里我们发现对于最内存循环k,没有存在依赖关系,具有并行基础,因此此处我们可以使用openMPI进行并行化,因此类似这处地方我们采用下面的方式进行优化:
```c++
for(i=1;i<p;i++){
    for(j=0;j<m;j++){
    MPI_Recv(&a(j,0),M,MPI_FLOAT,i,j,MPI_COMM_WORLD,&status);
    #pragma omp parallel for private(i,j,k) shared(A,a,p)
    for(k=0;k<M;k++)
        A((j*p+i),k)=a(j,k);
}
```
#### <a name=':-1'></a>运行截图:
![](./pic/18.png)
上图是算法在单核单线程的情况下运行的结果

![](./pic/16.png)
上图是算法在4核单线程的情况下运行的结果

![](./pic/17.png)
上图是算法在4核4线程的情况下运行的结果

我们可以得出使用多核进行计算时,可以极大提升计算性能,在此基础上使用omp可以进一步小幅优化,但是幅度并不明显(实际实验中本人更换参数,发现即使使用不同参数omp起到的作用都不明显)

### <a name='QR'></a>实验二(QR分解)
#### <a name='-1'></a>题目
针对教材中 18.6 节 QR 分解，给出纯 OpenMP 实现
#### <a name='-1'></a>代码实现
整体代码同样不短,因此这里我们仅对重要部分进行摘取:
```c++
#pragma omp parallel shared(Q, A, condition) private(my_thread, total_thread, base, block)
    {
        my_thread = omp_get_thread_num();
        total_thread = omp_get_num_threads();
        block = N / total_thread;
        base = my_thread * block;
        for (int j = 0; j < base + block; ++j)
        {
            if (j < base)
            {
                 wait(j, base, condition);
                transform(j, base, base + block);
            }
            else
            {
                transform(j, j + 1, base + block);
            }
            condition[j] = base + block;
        }
    }
```
上面代码为已经使用omp优化后的代码,主要的思路在于内循环j没有依赖关系,可以使用omp进行并行执行分块的求解
#### <a name='-1'></a>运行截图
![](./pic/21.png)
上述为单核单线程的运行截图
![](./pic/20.png)
上述为单核8线程的运行截图
![](./pic/19.png)
上述为单核16线程的运行截图

因此我们不难看出随着开辟的线程数变多,性能也有一定的提升,尽管提升并不是线性的

### <a name='summa'></a>实验三(summa)
#### <a name=':-1'></a>题目:
SUMMA 并行矩阵乘法的 MPI 实现。参看文件 summa_2010.pdf 和原始文 章 lawn96.pdf。

#### <a name=':-1'></a>代码实现:
同样由于代码过长,因此简单简述主要算法思想(主要借用ppt)
![](./pic/22.png)
因此其可以表示为下面伪代码的形式
![](./pic/23.png)
```c++
对所有进程(pos_I,pos_J)，执行：
for( k = 0; k<L, k++ ) {
    所有的处理器行I ：A(I,k)的所有者在该处理器行内广播A子块中的该列；
    所有的处理器列J ：B(k,J)的所有者在该处理器列内广播B子块中的该行；
    接收来自处理器行pos_I的A(pos_I,k)到Acol；
    接收来自处理器列pos_J的B(k,pos_J)到Brow；
    C(pos_I,pos_J) = C(pos_I,pos_J) + Acol * Brow
}
```

#### <a name='-1'></a>运行截图
![](./pic/23.png)
上图为使用单核情况下程序对于1024*1024问题求解的运行截图
![](./pic/24.png)
上图为使用4核情况下对相同规模任务求的运行时间
![](./pic/25.png)
上图为使用16核对相同问题求解的运行时间

我们可以发现随着显成熟的提升,传送数据所需时间也会上升,相对应的计算C的时间会下降,因此需要在其中找到一个平衡,找到合适的线程数使得总体时间(传输时间+计算时间)最小化


### <a name='-'></a>实验四(自选命题-随机算法并行实现)
#### <a name='-1'></a>题目
本次自定义实验本人选择了最近学到的概率算法中的一个题目作为选题,算法的大致流程如下:
使用随机撒点的方式计算pi的近似值,原理如下:
>+ 随机生成点坐标(x,y),随机算法保证x,y为(0,1)的均匀分布
>+ 判断点落入圆内/圆外
>+ 重复多次后,记录总重复次数为sum,落入圆内次数为hit,则pi可以表示为:
$$
\pi = \frac{4*hit}{sum}
$$

首先一个显然的性质就是,重复撒点的次数越多,则算出来的pi越接近真实值,而每次撒点均是独立无关的因此很自然的我们可以开多线程,并行的进行撒点试验,之后将各自的结果送回到rank为0的节点计算最后的结果

#### <a name='-1'></a>代码实现
```c++
#include <bits/stdc++.h>
#include <random>
#include <time.h>
#include <mpi.h>
using namespace std;
const int maxn = 1e6;
int A[100];
int main(int argc, char **argv)
{
    int rank;
    int size;
    MPI_Status status;
    MPI_Init(&argc, &argv);                 //初始化进程
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size); 
    
    clock_t begin = clock();
    default_random_engine random(time(NULL));           //定义随机数生成器
    uniform_real_distribution<double> uniform(0.0,1.0);

    for(int i=0;i<maxn;i++){                            //随机撒点实验
        double x = uniform(random);
        double y = uniform(random);
        if(x*x+y*y<=1) A[rank]++;
    }

    if(rank==0){                                        //rank为0的节点负责搜集信息,并计算最后pi值并输出
        for(int i=1;i<size;i++) MPI_Recv(A+i,1,MPI_INT,i,0,MPI_COMM_WORLD,&status);
        int sum = 0;
        for(int i=0;i<size;i++)  sum += A[i];

        printf("tot:%d hit:%d pi:%lf 相对误差:%lf %%\n",maxn*size,sum,4.0*sum/maxn/size,abs(4.0*sum/maxn/size-3.1415926535)/3.1415926535*100);
        
    }else {                                             //其他节点将自己的实验结果送给rank为0的节点
        MPI_Send(A+rank,1,MPI_INT,0,0,MPI_COMM_WORLD);
    }

    MPI_Finalize();
    clock_t end = clock();
    if (rank == 0){
        printf("Runtime:%lf(ms)\n",1000.0*(end-begin)/CLOCKS_PER_SEC);
    }

    return 0;
}
```

#### <a name='-1'></a>运行截图
![](./pic/14.png)
上图为单核运行的结果

![](./pic/15.png)
上图为使用4核计算的结果,在时间代价没有显著增加的情况下提高了重复次数,进而提高了精度
