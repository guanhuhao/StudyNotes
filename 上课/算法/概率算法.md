## 概率算法
### 1 引论
#### 期望事件与平均时间:
>+ 确定算法的平均执行时间:输入规模一定的所有输入实例是**等概率出现**时，算法的平均执行时间。
>+ 概率算法的期望执行时间:**反复解同一个输入**实例所花的平均执行时间。

因此对于概率算法有下面两种期望:
>+ 平均的期望时间：所有输入实例上平均的期望执行时间 
>+ 最坏的期望时间：最坏的输入实例上的期望执行时间

#### 概率算法的特点
>+ 不可再现性:在同一个输入实例上，每次执行结果不尽相同.可能的原因有下面两种
>+ 分析困难:需要有概率论,统计学和数论知识.

#### uniform函数
我们约定该函数随机,均匀,独立选取值其中:
>+ uniform(a,b):表示从$[a,b)$**实数区间**中挑选一个随机实数
>+ uniform(i..j):表示从$[a,b]$**整数区间**中挑选一个一个随机整数
>+ uniform(X)$\in$ X:表示从**非空集合X**中挑选一个X的元素

### 1.2 概率算法的分类:
#### 基本特征(随机决策):
>+ 在同一实例上执行两次其结果(或过程)可能不同
>+ 在同一实例上执行两次的时间亦可能不太相同

#### 分类:
大体上分成下面四类:
>+ Numerical(数字算法)
>+ Monte Carlo(蒙特卡洛)
>+ Las Vegas
>+ Sherwood

但很多人将所有概率算法(尤其是数字的概率算法) 称为Monte Carlo算法
**数字算法:**
随机性被最早用于求数字问题的近似解 
>+ 概率算法获得的答案一般是近似的，但通常算法执行的时间越长，精度就越高，误差就越小 
>+ 使用的理由:
>>+ 现实世界中的问题在原理上可能就不存在精确解,例如:实验数据本身就是近似的，一个无理数在计算机中只能近似地表示
>>+ 精确解存在但无法在可行的时间内求得 有时答案是以置信区间的形式给出的

**Monte Carlo算法 (MC算法)**
蒙特卡洛算法1945年由J. Von Neumann进行核武模拟提出的。它是以**概率和统计**的理论与方法为基础的一种**数值计算**方法，它是双重近似：
>+ 一是用概率模型模拟近似的数值计算
>+ 二是用伪随机数模拟真正的随机变量的样本。 

这里我们指的MC算法是： 若问题**只有1个正确的解**，而无近似解的可能时使用MC算法
**特点：** MC算法总是给出一个答案，但该答案未必正确，成功(即答案是正确的)的概率正比于算法执行的时间 
**缺点：** 一般不能有效地确定算法的答案是否正确

**Las Vegas算法 (LV算法)** 
LV算法**绝不返回错误的答案**。
特点：获得的**答案必定正确**，但有时它仍根本就找不到答案。 
和MC算法一样，成功的概率亦随算法执行时间增加而增加。无论输入何种实例，只要算法在该实例上运行足够的次数，则算法失败的概率就任意小。

**Sherwood算法**
**总是**给出正确的答案。 当某些确定算法解决一个特殊问题**平均的时间比最坏时间快得多**时，我们可以使用Sherwood算法来减少，甚至是消除好的和坏的实例之间的差别。

### 2.数字概率算法
#### 定积分的概率算法1(hitormiss 算法)
若$I$为$\int_0^1 f(x)dx$的正确值,h时随机算法返回的值,当$n>I(1-I)/\epsilon^2\delta$时有:
$$Prob(|h-I|<\epsilon)\geq1-\delta$$
即当$n>I(1-I)/\epsilon^2\delta$时有算法的计算结果的绝对误差超过ε的概率不超过δ，因此我们根据给定ε和δ可以确定算法迭代的次数.
解决该问题时可是用切比雪夫不等式,将I看成期望

#### 定积分的概率算法2(crude 算法)
在积分区间上随机均匀地产生点，求出这些点上的函数值的算术平均值，再乘以区间的宽度,其积分式可改写为：
$$\int_a^bf(x)dx=(b-a)\frac{1}{n}\sum_{i-1}^nf(x_i),a\leq x_i\leq b$$
算法流程(伪代码如下):
```c++
Crude (f, n, a, b) { 
    sum ← 0; 
        for i ← 1 to n do { 
            x ← uniform(a, b); 
            sum ← sum + f(x); 
            }
    return (b-a)sum/n;
}
```
对于给定的迭代次数n，Crude算法的方差不会大 于HitorMiss的方差。但不能说，Crude算法总是 优于HitorMiss。因为后者在给定的时间内能迭代 的次数更多。例如，计算π值时，Crude需计算 平方根，而用投镖算法darts时无需计算平方根。

#### 定积分的确定算法(梯形算法)
将区间分为n-1个子区间，每个子区间内的长度为δ，类似的其积分式可改写为:
$$\int_a^bf(x)dx=\delta(f(a+\delta)+f(a+2\delta)+...+\frac{f(a)+f(b)}{2})$$
相当于将区间划分为n-2个区间,最后一个可能缺少的区间使用$\frac{f(a)+f(b)}{2}$代替
其算法流程可表示为:
```c++
Trapezoid (f, n, a, b) { // 假设 n ≥ 2 
    delta ← (b-a)/(n-1); 
    sum ← (f(a) + f(b))/2; 
    for x ← a+delta step delta to b – delta do 
        sum ← sum + f(x) 
    return sum × delta; 
}
```

一般的来说相同精度下梯形算法的迭代次数少于MC积分,但是有时候确定性积分算法求不出解,或者对于多重积分可能采样点过多导致无法计算

#### 2.3 概率计数
##### 求集合的势
设X是具有n个元素的集合，我们有回放地随机， 均匀和独立地从X中选取元素，设k是出现第1次重 复之前所选出的元素数目，则当n足够大时，k的期 望值趋近为$\beta \sqrt{n}$，这里:
$$\beta = \sqrt{\pi/2}=1.253$$
因此可以得到估计|X|的概率算法为:
$$\beta \sqrt x=\sqrt{n\pi/2}=k\\
\,\\
n=\frac{2k^2}{\pi}$$
其算法流程可写为:
```c++
SetCount (X) { 
    k ← 0; S ← Ф; 
    a ← uniform(X); 
    do { 
        k++; 
        S ← S U{a}; 
        a ← uniform(X); 
    } while (a ∉ S);
    return 2k2/π;
}
```
该算法期望的时间/空间复杂度为$\theta(\sqrt n)$,因为$k=\theta(\sqrt n)$

##### 多重集合不同对象数目估计
设U是单词序列的集合，设参数m稍大于lgM，可令$m=5+\lceil{lg M}\rceil $,设h为一个$U\to{0,1}^m$的散列函数,定义$\pi(y,b)$ b为0或者1,该函数表示第y中第一次出现b(0/1)的位置,其算法流程为:
```c++
WordCount () { y[1..m+1] ← 0; // 初始化 
    for 磁带上每个单词x do { //顺序读磁带 
        i ← π(h(x), 1); // x的散列值中等于1的最小位置，表示x是以00..01打头的 
        y[i] ← 1; // 将该位置置为1 
    }
    return π(y, 0); // 返回y中等于0的最小位置 
}
```
**上界估计:**
如果散列函数值足够平均,则可以通过返回的$\pi(y,0)$进行单词个数的估计,假设有m个单词,其第返回的位置为k,则其一个粗略的上界为$2^k$
**下界估计:**
同理,类似的可以得到对应的一个下界为$2^{k-2}$
**其他分析:**
磁带上互不相同的单词数目为：$2^{k-2}～2^k$ 
实际上，算法WordCount估计的n应等于2k/1.54703
性能：时间O(N)，空间：O(lgN)

### 3 sherwood算法
对于一个确定算法,其解决大小为n的实例平均执行时间可以表示为:
$$
\bar{t}_A(n)=\sum_{x\in X_n}t_A(x)/|X_n|
$$
但是仍可能存在对于某个大小为n的数据使得其所需计算时间远大于平均执行时间

对于一个概率算法,其平均执行时间可以写为:
$$
t_B(x)=\bar{t}_A(n)+s(n)
$$
其中s(n)为随机化取得均匀性所付出的代价,通常情况下远小于确定性算法计算时间

#### 3.2 随机的预处理
将一个算法改成sherwood算法的一般算法为:
>+ ① 将被解的实例变换到一个随机实例。// 预处理
>+ ② 用确定算法解此随机实例，得到一个解。 
>+ ③ 将此解变换为对原实例的解。 // 后处理

对于解决某问题的函数$f:X\to Y$,随机的预处理可以由一对函数构成:
$$
u:X_{old}\times A \to X_{New}\\
y:A\times Y_{New} \to Y_{old}
$$
u,v满足下面三个性质:
>+ 原实例x可通过随机抽样变换成另一个实例
>+ 对y的解可变换为对原实例x的解
>+ 函数u,v在最坏的情况均能有效计算


### 4.Las Vegas 算法
**Las Vegas算法特点:**
>+ 可能不时地要冒着找不到解的风险，算法要么返回正确的解， 要么随机决策导致一个僵局。 
>+ 若算法陷入僵局，则使用同一实例运行同一算法，有独立的 机会求出解。 
>+ 成功的概率随着执行时间的增加而增加
>+ Las Vegas算法一般能获得更有效率的算法,甚至有时对于每个实例皆如此,但是Las Vegas的时间上界可能不存在

**算法的一般形式:**
|数学符号|含义|
|---|---|
|$LV(x,y,success):$|x是输入的实例，y是返回 的参数，success是布尔值，true表示成功，false 表示失败|
|$p(x)$|对于实例x，算法成功的概率|
|$s(x)$|算法成功时的期望时间|
|$e(x)$|算法失败时的期望时间|

一个正确的算法要求对于每个实例,$p(x)>0$
