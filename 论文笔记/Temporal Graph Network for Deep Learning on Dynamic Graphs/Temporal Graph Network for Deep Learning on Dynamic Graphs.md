## Temporal Graph Networks for Deep Learning on Dynamic Graph

### 1.Contribution 
>+ 1. 提出了一个同意的框架用于处理由一系列事件组成的连续时间上的动态图问题
>+ 2. 提出了一个新的训练策略在保证并行处理的时候从序列中学习信息

### 2.Background
对于静态图上的学习:
传统算法对于单个节点的嵌入表示通常使用下面的规则:
$$z_i= \sum_{j\in n_i}h(m_{ij},v_j)$$
其中$m_{i,j}$表示从邻居节点j传到i的消息即$m_{ij}=msg(v_i,v_j,e_{ij})$,而h表示一个可学习的函数

对于动态图上的学习:
主要分成两种类型:
>+ 1.基于离散时间的动态图,每隔一段时间对图进行快照,作为该段时间间隔内图的变化
>+ 2.基于连续时间的动态图,将图变化视为一系列连续时间的事件(如边/点的删除/增加,特征变化)

本文主要研究基于连续事件的动态图变化,其中对于事件定义了下面三种:
>+ 1. 节点级别事件,使用一个节点向量进行表述,其中v[i]种i表示一个节点,如果不存在则创建,如存在则更新
>+ 2. 相互作用事件(边),可能存在重边
>+ 3. 删除事件

### 3.时序图神经网络
对于时序图神经网络,可以视为一个解码器以及一个编码器的组合,编码器是一个将动态图结构映射到一个节点嵌入表示的函数,解码器将一个或者几个节点表示作为输入,进行目标任务的预测.
本文种最大的贡献在于提出了一个针对连续事件的时序图神经网络的编码方式,即将事件作为一系列具有时间戳的事件,同时定义图节点嵌入为$Z(t)=(z_1(t),...,z_{n(t)}(t))$

#### 3.1核心模块
##### Memory(记忆方式)
对于模型在t时刻的状态储存为一个当前所有已经出现节点组成的向量,该记忆状态会在事件之后更新,可以理解为节点历史状态的压缩记忆存储,该模块是是时序图神经网络具有能力进行长期依赖记忆能力的前提,对于新加入的节点其初始向量为0,且会被后续事件进行更新

##### Message Function(消息函数)
对于所有涉及节点i的事件(event),都会使用信息更新i的记忆表示,对于一个相互作用事件的两个顶点u,v,不妨记消息由u传播到v,则有下面的记忆更新方式:
$$
m_i(t) = msg_s(s_i{t^-},s_j(t^-),\Delta t,e_{ij}(t))
m_j(t) = msg_s(s_j{t^-},s_i(t^-),\Delta t,e_{ij}(t))
$$

参数解释:$s_i(t^{-})$表示节点i在t时刻前的记忆表示,$msg_s,msg_d,msg_n$为可学习的消息函数

##### Message Aggregator(消息聚集)
为了提升性能采取的批处理,可能会导致在同一批处理中涉及对单个节点的多次事件,因此本文将这种情况进行一个消息的聚集:
$$
\bar{m}_i(t)=agg(m_i(t_1),...,m_i(t_b))
$$
本文采用了两种简单的非学习的方式进行消息的聚集:1.仅使用最近消息,2.使用消息的平均值

##### Memory Updater(记忆更新机制)
当事件发生后,更新对应涉及的节点记忆,即:
$$
s_i(t)=mem(\bar m_i(t),s_i(t^-))
$$
此处mem是一个可学习的更新函数如LSTM,GRU

##### Embedding嵌入
生成节点的时序嵌入,目的在于避免内存生锈问题(? Kazemi 2020提出的),文中以社交网络中可能出现的某人长时间不使用账号导致其账号一直无效占用内存,该模块可以实现的方式很多,本文中使用下面的方式:
$$
z_i(t)=emb(i,t)=\sum_{j\in n^k_i([0,t])}h(s_i(t),s_j(t),e_{i,j},v_i(t),v_j(t))
$$
其中h为可学习的函数,包含了很多不同的公式如:
>+ Identity(id):$emb(i,t)=s_i(t)$表示某个点在t时刻时的嵌入表示
>+ Time projection(时间映射):$emb(i,t)=(1+\Delta tw)\cdot s_i(t)$w是可学习的参数$\Delta t$表示最后以此更新的时间间隔,$\cdot$ 操作表示元素级别的向量乘积,该嵌入方式由Kumar在2019年于Jodie中提出
>+ Temporal Graph Attention(时序图注意力机制):使用一系列的图注意力层计算节点i的嵌入表示,其使用节点的L跳内的邻居进行消息的聚集

对于节点i第l层的输入$h_i^{(l-1)}(t)$,t表示当前时间戳,$\{h_1^{(t-1)}(t),...,h_N^{(t-1)}(t)\}$表示节点i的邻居节点表示,其对应的事件戳以及特征分别用$t_1,...,t_N$以及$e_{i1},...,e_{iN}(t_N)$表示,因此对于下面有这些式子:
$$
\begin{aligned}
&h_i^{(l)}(t) = MLP^{(l)}(h_i^{(l-1)}(t)||\tilde{h}_i^{(l)}(t)),\\
&\tilde{h}_i^{(l)}(t) = MultiHeadAttention^{(l)}(q^{(l)}(t),K^{(l)}(t),V^{(l)}(t)).\\
&q^{(l)}(t)=h_i^{(i-1)}(t)||\phi(0),\\
&K^{(l)}(t) = V^{(l)}(t) = C^{(l)}(t),\\
&C^{(l)}(t) = [h_1^{(l-1)}||e_{i1}(t)||\phi(t-t_1),...,h_N^{i-1}(t)||e_{iN}(t_N)||\phi(t-t_N)] 
\end{aligned}
$$
参数解释:
>+ $\phi(\cdot)$:为Xu在2020 年突出的一个普遍的时间编码器
>+ $||$:表示矩阵级联操作
>+ $z_i(t)=emb(i,t)=h_i^{(L)}$:表示点i在t时刻的嵌入表示
>+ $q^{(l)}(t):$为对于相关节点的询问操作(?举例为目标节点或其L-1条的邻居)
>+ $K^{(l)}(t),V^{(l)}(t)$:分别表示邻居节点的key以及value值
>+ MLP:表示将节点信息于聚集的meg消息合并的函数

对于初始状态$h_j^{(0)}$的设置,加入了节点级别的时序特征即
$$
h_j^{(l)}(t) = s_j(t)+v_j(t)
$$
其中$s_j(t)$表示节点特征(即节点记忆表示),$v_j(t)$则表示节点的时序特征

**Temporal Graph Sum:** 通过一个简单且快速的方式获取整张图的聚集信息:
$$
\begin{aligned}
&h_i^{(i)}(t) = W_2^{(l)}(h_i^{(l-1)}(t)||\tilde{h}_i^{(l)}(t)),\\
&\tilde{h}_i^{(l)}(t) = ReLu(\sum_{j\in n_i([0,t])}W_1^{(l)}(h_j^{l-1}(t)||e_{ij}||\phi(t-t_j)))
\end{aligned}
$$
参数解释:
>+ $\phi (\cdot)$ :为一个时间编码器
>+ $z_i(t):$同上

本文中使用的时序图注意力机制,以及时序图sum分别使用2019年 Kazemi的Time2Vec以及2020年Xu的TGAT模型

其中使用的图嵌入技术通过使用对其邻居节点的记忆来解决记忆生锈问题,当一个节点一段时间不被激活则之后更可能使用其被激活的邻居节点的记忆来进行更新,TGN模型计算最新的节点嵌入,而注意力机制则根据特征以及时间信息选择更重要的节点进行更新

#### 3.2 训练过程
使用TGN进行多种任务的计算如自监督的边预测,半监督的节点分类问题,对于连接预测问题使用一系列按时间排序的中间过程,训练目标设定为根据过往记录预测未来的迭代的变化.
训练过程中最耗时的部分为与记忆相关的模块(如消息函数,消息聚集,记忆更新),原因在于不直接影响loss.因此不存在梯度,为了解决该问题,记忆必须先于预测相同的相互作用(interaction)之前,但是先于相同interaction更新时可能会出现消息泄露(infomation leakage),为了避免该问题当处理某一批时,将上一批中传递的msg进行对应的记忆更新,Figure2展示了这一过程
对于任意时刻t,对于每个节点仅储存一个还未处理的消息,该消息由涉及到节点i的最近的一次相互作用产生,当模型处理下一个关系时,将该节点的记忆通过这个消息进行更新并计算loss,对于批次大小的选择,当我们一批次取少量元素进行更新时,会导致效率过慢,而一批次选取大量元素时,会出现无法使用最新的消息来进行样本的更新,使得准确率下降,因此本文在效率以及准确性中进行平衡原则大约200左右作为一批次的样本数量.

### 代码分析
对于数据部分将数据分为下面几种:
>+ node_features:节点特征,对于wiki数据集有9000+个点,每个点有172个特征,但是均为0,可能当同构节点处理
>+ edge_features:边特征,有157475个点,每个点有172个特征,特征有103558种不同取值
>+ full_data:全部数据,涉及157474条关系,9227个不同的节点
>+ train_data:训练数据,涉及81029条关系,6147个不同节点
>+ val_data:验证数据,涉及23621条关系,3256个不同节点
>+ test_data:测试数据,涉及23621条关系,3564个不同节点
>+ new_node_val_data:目前不明,涉及12016条关系,2120个不同的节点
>+ new_node_test_data:目前不明,涉及11715条关系,2437个不同的节点

对于Data类有以下成员变量:
>+ source:关系(边)的出点,为向量表示,向量长度等于关系数
>+ destination:关系(边)的入点,为向量表示,向量长度等于关系数
>+ timestamp:时间戳,为向量表示,向量长度等于关系数
>+ edge_idex:边id,为向量表示,向量长度等于关系数
>+ labels:emmm,还得看看
>+ n_interactions:涉及的关系数,可以简单理解为数据条数
>+ unique_nodes:所有关系涉及的点数目
